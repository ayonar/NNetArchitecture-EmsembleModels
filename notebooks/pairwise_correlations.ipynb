{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _tutorial_pairwise_correlations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Very Large Correlation Matrices in Parallel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "[:download:`ipython notebook <pairwise_correlations.ipynb>`] [:download:`python script <pairwise_correlations.py>`]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "In this short tutorial, we'll demonstrate how DeepGraph can be used to efficiently compute very large correlation matrices in parallel, with full control over RAM usage.\n",
    "\n",
    "Assume you have a set of ``n_samples`` samples, each comprised of ``n_features`` features and you want to compute the `Pearson correlation coefficients <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_ between all pairs of samples. If your data is small enough, you may use `scipy.stats.pearsonr <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr>`_ or `numpy.corrcoef <https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html>`_, but for large data, neither of these methods is feasible. Scipy's pearsonr  would be very slow, since you'd have to compute pair-wise correlations in a double loop, and numpy's corrcoef would most likely blow your RAM.\n",
    "\n",
    "Using DeepGraph's :py:meth:`create_edges <.create_edges>` method, you can compute all pair-wise correlations efficiently. In this tutorial, the samples are stored on disc and only the relevant subset of samples for each iteration will be loaded into memory by the computing nodes. Parallelization is achieved by using python's standard library `multiprocessing <https://docs.python.org/3.6/library/multiprocessing.html>`_, but it should be straight-forward to modify the code to accommodate other parallelization libraries. It should also be straight-forward to modify the code in order to compute other correlation/distance/similarity-measures between a set of samples. \n",
    "\n",
    "First of all, we need to import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data i/o\n",
    "import os\n",
    "\n",
    "# compute in parallel\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# the usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import deepgraph as dg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a set variables and store it as a 2d-matrix ``X`` (``shape=(n_samples, n_features)``) on disc. To speed up the computation of the Pearson correlation coefficients later on, we whiten each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create observations\n",
    "from numpy.random import RandomState\n",
    "prng = RandomState(0)\n",
    "n_samples = int(1e2)\n",
    "n_features = int(1e1)\n",
    "X = prng.randint(100, size=(n_samples, n_features)).astype(np.float64)\n",
    "\n",
    "Xvar = X.var(axis=1, keepdims=True, ddof=1)\n",
    "# whiten variables for fast parallel computation later on\n",
    "X = X - X.mean(axis=1, keepdims=True)\n",
    "#X = (X - X.mean(axis=1, keepdims=True)) / X.std(axis=1, keepdims=True)\n",
    "\n",
    "# save in binary format\n",
    "np.save('samples', X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (change these to control RAM usage)\n",
    "step_size = 1e5\n",
    "n_processes = 100\n",
    "\n",
    "# load samples as memory-map\n",
    "X = np.load('samples.npy', mmap_mode='r')\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "# connector function to compute pairwise pearson correlations\n",
    "def corr(index_s, index_t):\n",
    "    samples_s = X[index_s]\n",
    "    samples_t = X[index_t]\n",
    "    corr = np.einsum('ij,ij->i', samples_s, samples_t) / (n_features-1)\n",
    "    return corr\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_samples*(n_samples-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# parallel computation\n",
    "def create_ei(i):\n",
    "\n",
    "    from_pos = pos_array[i]\n",
    "    to_pos = pos_array[i+1]\n",
    "\n",
    "    # initiate DeepGraph\n",
    "    g = dg.DeepGraph(v)\n",
    "\n",
    "    # create edges\n",
    "    g.create_edges(connectors=corr, step_size=step_size, \n",
    "                   from_pos=from_pos, to_pos=to_pos)\n",
    "\n",
    "    # store edge table\n",
    "    g.e.to_pickle('/tmp/corr/{}.pickle'.format(str(i).zfill(3)))\n",
    "\n",
    "# computation\n",
    "if __name__ == '__main__':\n",
    "    indices = np.arange(0, n_processes - 1)\n",
    "    p = Pool()\n",
    "    for _ in p.imap_unordered(create_ei, indices):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now we can compute the pair-wise correlations using DeepGraph's :py:meth:`create_edges <.create_edges>` method. Note that the node table :py:attr:`v <.DeepGraph.v>` only stores references to the mem-mapped array containing the samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect the computed correlation values and store them in an hdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store correlation values\n",
    "files = os.listdir('/tmp/corr/')\n",
    "files.sort()\n",
    "store = pd.HDFStore('e.h5', mode='w')\n",
    "for f in files:\n",
    "    et = pd.read_pickle('/tmp/corr/{}'.format(f))\n",
    "    store.append('e', et, format='t', data_columns=True, index=False)\n",
    "store.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-128.16666667,  664.76666667,  -57.1       , -242.88888889,\n",
       "        344.14444444,  198.06666667,  183.72222222, -223.92222222,\n",
       "        -79.34444444, -696.75555556,  267.26666667,  -58.83333333,\n",
       "          7.33333333,   10.11111111,  186.41111111,  282.25555556,\n",
       "         60.18888889,  461.02222222, -419.5       ,  -13.44444444,\n",
       "        -94.74444444, -116.77777778,   36.17777778,   54.5       ,\n",
       "        139.48888889,  130.56666667, -232.64444444,  461.13333333,\n",
       "        110.64444444, -179.08888889, -133.72222222, -118.07777778,\n",
       "       -202.57777778,   52.38888889,  -41.24444444, -129.94444444,\n",
       "         18.92222222, -259.22222222,  214.82222222, -221.32222222,\n",
       "       -100.41111111, -106.51111111,  415.01111111,  278.74444444,\n",
       "       -124.97777778,  442.15555556,   49.72222222, -109.37777778,\n",
       "       -358.42222222,   -9.71111111, -125.24444444,  -80.18888889,\n",
       "       -158.85555556, -189.27777778,  523.5       ,  -88.9       ,\n",
       "        -42.85555556,  213.87777778,  338.2       ,  120.64444444,\n",
       "       -204.33333333, -155.76666667, -137.14444444,  366.45555556,\n",
       "        403.61111111,  -51.28888889, -181.66666667,  363.06666667,\n",
       "         78.46666667, -269.01111111, -207.47777778, -135.77777778,\n",
       "        414.72222222,   64.87777778, -128.54444444,  229.57777778,\n",
       "        236.98888889,  -66.16666667,  -49.68888889, -292.37777778,\n",
       "       -115.81111111,  131.31111111,  -35.76666667,  333.96666667,\n",
       "        -57.9       ,   83.24444444,   80.02222222,  -81.07777778,\n",
       "       -500.7       , -295.33333333, -229.56666667, -135.17777778,\n",
       "       -317.76666667,   60.5       ,  114.74444444,  -88.1       ,\n",
       "       -556.46666667,   85.76666667, -377.45555556,   72.78888889])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             corr\n",
      "s  t             \n",
      "0  1  -128.166667\n",
      "   2    93.500000\n",
      "   3   136.333333\n",
      "   4   -83.611111\n",
      "   5    59.111111\n",
      "   6  -128.722222\n",
      "   7   -52.500000\n",
      "   8     4.166667\n",
      "   9   334.888889\n",
      "   10 -239.222222\n",
      "   11 -198.277778\n",
      "   12 -421.555556\n",
      "   13  215.666667\n",
      "   14  -33.611111\n",
      "   15   79.500000\n",
      "   16  -17.388889\n",
      "   17  268.777778\n",
      "   18   70.388889\n",
      "   19   58.222222\n",
      "   20   10.166667\n",
      "   21 -286.555556\n",
      "   22 -108.888889\n",
      "   23   63.611111\n",
      "   24 -189.666667\n",
      "   25   62.166667\n",
      "   26  -81.666667\n",
      "   27 -113.555556\n",
      "   28  225.111111\n",
      "   29    2.888889\n",
      "   30 -319.944444\n",
      "...           ...\n",
      "91 98 -295.955556\n",
      "   99 -377.488889\n",
      "92 93   42.722222\n",
      "   94 -138.633333\n",
      "   95  180.100000\n",
      "   96  435.355556\n",
      "   97  151.900000\n",
      "   98  156.566667\n",
      "   99   13.877778\n",
      "93 94 -437.833333\n",
      "   95  238.833333\n",
      "   96  117.222222\n",
      "   97 -329.722222\n",
      "   98  291.166667\n",
      "   99   25.611111\n",
      "94 95 -172.744444\n",
      "   96 -322.066667\n",
      "   97   77.522222\n",
      "   98  172.188889\n",
      "   99  107.255556\n",
      "95 96  106.466667\n",
      "   97  100.566667\n",
      "   98  128.900000\n",
      "   99  -84.344444\n",
      "96 97 -334.800000\n",
      "   98  132.422222\n",
      "   99 -309.866667\n",
      "97 98 -528.455556\n",
      "   99  425.788889\n",
      "98 99 -150.433333\n",
      "\n",
      "[4950 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# load correlation table\n",
    "e = pd.read_hdf('e.h5')\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's see where most of the computation time is spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "g = dg.DeepGraph(v)\n",
    "p = %prun -r g.create_edges(connectors=corr, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         7850 function calls (6521 primitive calls) in 0.013 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 506 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    202/4    0.001    0.000    0.003    0.001 abc.py:194(__subclasscheck__)\n",
      "        2    0.001    0.000    0.001    0.000 {built-in method posix.stat}\n",
      "        2    0.000    0.000    0.001    0.000 memmap.py:334(__getitem__)\n",
      "      438    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
      " 1221/728    0.000    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "      186    0.000    0.000    0.001    0.000 _weakrefset.py:58(__iter__)\n",
      "       47    0.000    0.000    0.000    0.000 tokenize.py:494(_tokenize)\n",
      "   196/22    0.000    0.000    0.003    0.000 typing.py:1166(__subclasscheck__)\n",
      "        2    0.000    0.000    0.000    0.000 internals.py:4801(_stack_arrays)\n",
      "  508/118    0.000    0.000    0.003    0.000 {built-in method builtins.issubclass}\n",
      "    50/48    0.000    0.000    0.000    0.000 common.py:1773(_get_dtype_type)\n",
      "      112    0.000    0.000    0.000    0.000 _weakrefset.py:81(add)\n",
      "      110    0.000    0.000    0.000    0.000 _weakrefset.py:26(__exit__)\n",
      "  313/270    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "      110    0.000    0.000    0.000    0.000 _weakrefset.py:20(__enter__)\n",
      "     30/8    0.000    0.000    0.002    0.000 typing.py:898(__extrahook__)\n",
      "        1    0.000    0.000    0.007    0.007 deepgraph.py:4783(_matrix_iterator)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get_labels' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
      "        1    0.000    0.000    0.003    0.003 deepgraph.py:5289(_select_and_return)\n",
      "        2    0.000    0.000    0.001    0.000 algorithms.py:438(factorize)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1070d8a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most of the time is spent by getting the requested samples in the corr-function, followed by computing the correlation values themselves. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}